{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from source.datasets import get_BCSS,CustomDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from source.vision_transformer import vit_small, vit4k_xs\n",
    "from source.utils import update_state_dict\n",
    "from utils.dbscan_utils import get_core_expert,get_metrics, get_weights,get_noisy\n",
    "from utils.distance_utils import calculate_distances\n",
    "from utils.patching_utils import get_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "    print(device)\n",
    "\n",
    "dir = '/datadisk/datasets/breast/expert'\n",
    "experts = ['expert','NP20','NP6','NP8','NP18','NP16','NP4']\n",
    "\n",
    "img = ''\n",
    "patch_size = 256\n",
    "region_size = 4096\n",
    "mini_patch_size = 16\n",
    "n_classes = 4 #1,2,3,4\n",
    "json_file = 'kfolds.json'\n",
    "checkpoint_256 = '/checkpoints/vit256_small_dino.pth'\n",
    "\n",
    "imgs = get_BCSS(dir)\n",
    "imgs_dataset = CustomDataset(imgs, False)\n",
    "\n",
    "imgs_loader = torch.utils.data.DataLoader(imgs_dataset, batch_size=1, shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SET MODELS ####\n",
    "vit_patch = vit_small(\n",
    "    img_size=patch_size,\n",
    "    patch_size=mini_patch_size,\n",
    "    embed_dim=384,\n",
    "    mask_attn=False,\n",
    "    num_register_tokens=0,\n",
    ")\n",
    "\n",
    "\n",
    "state_dict = torch.load(checkpoint_256, map_location=\"cpu\",weights_only=False)\n",
    "checkpoint_key = \"teacher\"\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(vit_patch.state_dict(), state_dict)\n",
    "vit_patch.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_patch.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_patch.to(device)\n",
    "vit_patch.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GET FEATURES ####\n",
    "features = []\n",
    "for _,img in imgs_loader:\n",
    "        img = img.to(device)\n",
    "        feat = get_patches(img,vit_patch,patch_size,mode='features', region=False)\n",
    "\n",
    "        features.extend(feat.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sc_expert(expert,features):\n",
    "    train_data = get_BCSS(dir,expert,mode=\"label\")\n",
    "    #### GET FEATURES ####\n",
    "    labels = []\n",
    "    for label in train_data[0]:\n",
    "            label = get_patches(torch.from_numpy(label).to(device),vit_patch,patch_size,region_size,mode=\"label\",region=False)\n",
    "            labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "    features_aux = []\n",
    "    labels_aux = []\n",
    "    idx_aux = []\n",
    "\n",
    "    for i,lbl in enumerate(labels):\n",
    "        feat = features[i]\n",
    "        for j,label in enumerate(lbl):\n",
    "                label[label==5]=10\n",
    "                if len(np.unique(label))==1 and np.unique(label)[0]==10:\n",
    "                    continue\n",
    "                else:\n",
    "                    features_aux.append(feat[j])\n",
    "                    idx_aux.append(f'{i}_{j}')\n",
    "                    labels_aux.append(np.unique(label))\n",
    "\n",
    "\n",
    "    #### PCA ####\n",
    "    pca = PCA(n_components=0.9)\n",
    "    principalComponents = pca.fit_transform(features_aux)\n",
    "\n",
    "    knn_features = []\n",
    "    knn_labels = []\n",
    "    knn_idx = []\n",
    "    for i,lbl in enumerate(labels_aux):\n",
    "\n",
    "        knn_features.append(principalComponents[i])\n",
    "        knn_idx.append(idx_aux[i])\n",
    "        knn_labels.append(lbl)\n",
    "            \n",
    "    ###### DIVIDE BY CLASSES #####\n",
    "    cl_0 = {}\n",
    "    cl_1 = {}\n",
    "    cl_2 = {}\n",
    "    cl_3 = {}\n",
    "    cl_4 = {}\n",
    "\n",
    "    for i, idx in enumerate(knn_idx):\n",
    "        div_index = idx.split('_')\n",
    "        expert_ann = labels[int(div_index[0])][int(div_index[1])]\n",
    "        cl,count = np.unique(expert_ann,return_counts=True)\n",
    "        cl_counts = dict(zip(cl,count))\n",
    "        if 0 in cl:\n",
    "            if cl_counts[0]/(256**2)>=0.65:\n",
    "                cl_0[idx] = knn_features[i]\n",
    "        if 1 in cl:\n",
    "            if cl_counts[1]/(256**2)>=0.85:\n",
    "                cl_1[idx] = knn_features[i]\n",
    "        if 2 in cl:\n",
    "            if cl_counts[2]/(256**2)>0.85:\n",
    "                cl_2[idx] = knn_features[i]\n",
    "        if 3 in cl:\n",
    "            if cl_counts[3]/(256**2)>=0.85:\n",
    "                cl_3[idx] = knn_features[i] \n",
    "        if 4 in cl:\n",
    "            if cl_counts[4]/(256**2)>=0.85:\n",
    "                cl_4[idx] = knn_features[i] \n",
    "\n",
    "\n",
    "        \n",
    "        ###### DISTANCES #######\n",
    "        # dist0 = calculate_distances(cl_0)\n",
    "        dist1 = calculate_distances(cl_1)\n",
    "        dist2 = calculate_distances(cl_2)\n",
    "        dist3 = calculate_distances(cl_3)\n",
    "        dist4 = calculate_distances(cl_4)\n",
    " \n",
    "\n",
    "    ######## DBSCAN #########\n",
    "    centroids,labels,eps,min_samples = get_core_expert([cl_1,cl_2,cl_3,cl_4],[dist1,dist2,dist3,dist4])\n",
    "    sc,sc2,eucl_distances,min_dist,outliers,dists,means,stds,_ =  get_metrics(centroids,labels,return_neighbors=False)\n",
    "    noisy = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            noisy.append(get_noisy(label))\n",
    "        except:\n",
    "             noisy.append(0)\n",
    "    n_centroids = [centroid.shape[0] for centroid in centroids]\n",
    "\n",
    "\n",
    "    weights,percentage = get_weights(sc,outliers,dists, n_classes=n_classes)\n",
    "    np.save(f'{dir}/weights/weights_{expert}_v2.npy',weights)\n",
    "    \n",
    "    return [len(cl_1),len(cl_2),len(cl_3),len(cl_4)],n_centroids,noisy,sc,min_dist,\\\n",
    "        outliers,dists,means,stds,sc2,eucl_distances,weights,percentage,eps,min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cl_consensus,n_centroids_consensus,noisy_consensus,sc_consensus,min_edist_consensus,outliers_consensus,edists_consensus,\\\n",
    "means_consensus,stds_consensus,sc2_consensus,eucl_distance_consensus,\\\n",
    "    weights_consensus,percentage_consensus,eps_consensus,min_samples_consensus = get_sc_expert('expert',features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cl_NP2,n_centroids_NP2,noisy_NP2,sc_NP2,min_edist_NP2,outliers_NP2,edists_NP2,\\\n",
    "means_NP2,stds_NP2,sc2_NP2,eucl_distance_NP2,\\\n",
    "    weights_NP2,percentage_NP2,eps_NP2,min_samples_NP2 = get_sc_expert(experts[1],features)\n",
    "    \n",
    "len_cl_NP6,n_centroids_NP6,noisy_NP6,sc_NP6,min_edist_NP6,outliers_NP6,edists_NP6,\\\n",
    "means_NP6,stds_NP6,sc2_NP6,eucl_distance_NP6,\\\n",
    "    weights_NP6,percentage_NP6,eps_NP6,min_samples_NP6 = get_sc_expert(experts[2],features)\n",
    "\n",
    "len_cl_NP8,n_centroids_NP8,noisy_NP8,sc_NP8,min_edist_NP8,outliers_NP8,edists_NP8,\\\n",
    "means_NP8,stds_NP8,sc2_NP8,eucl_distance_NP8,\\\n",
    "    weights_NP8,percentage_NP8,eps_NP8,min_samples_NP8 = get_sc_expert(experts[3],features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cl_NP14,n_centroids_NP14,noisy_NP14,sc_NP14,min_edist_NP14,outliers_NP14,edists_NP14,\\\n",
    "means_NP14,stds_NP14,sc2_NP14,eucl_distance_NP14,\\\n",
    "    weights_NP14,percentage_NP14,eps_NP14,min_samples_NP14 = get_sc_expert(experts[4],features)\n",
    "\n",
    "len_cl_NP18,n_centroids_NP18,noisy_NP18,sc_NP18,min_edist_NP18,outliers_NP18,edists_NP18,\\\n",
    "means_NP18,stds_NP18,sc2_NP18,eucl_distance_NP18,\\\n",
    "    weights_NP18,percentage_NP18,eps_NP18,min_samples_NP18 = get_sc_expert(experts[5],features)\n",
    "\n",
    "len_cl_NP20,n_centroids_NP20,noisy_NP20,sc_NP20,min_edist_NP20,outliers_NP20,edists_NP20,\\\n",
    "means_NP20,stds_NP20,sc2_NP20,eucl_distance_NP20,\\\n",
    "    weights_NP20,percentage_NP20,eps_NP20,min_samples_NP20 = get_sc_expert(experts[6],features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
