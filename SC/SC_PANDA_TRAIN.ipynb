{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from source.datasets import get_PANDA,CustomDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from source.vision_transformer import vit_small, vit4k_xs\n",
    "from source.utils import update_state_dict\n",
    "import random \n",
    "from utils.dbscan_utils import get_core_expert,get_metrics,get_weights\n",
    "from utils.patching_utils import get_patches\n",
    "from utils.distance_utils import calculate_distances\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "    print(device)\n",
    "    \n",
    "random.seed(0)\n",
    "\n",
    "dir = '/datadisk/datasets/PANDA/'\n",
    "kfold = 'kfolds.json'\n",
    "img = ''\n",
    "patch_size = 256\n",
    "region_size = 4096\n",
    "mini_patch_size = 16\n",
    "checkpoint_256 = 'checkpoints/vit_256_small_dino_fold_4.pt'\n",
    "checkpoint_4k = 'checkpoints/vit_4096_xs_dino_fold_4.pt'\n",
    "\n",
    "expert = \"original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CustomDataset.__init__() missing 1 required positional argument: 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##### DATASET #####\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m get_PANDA(\u001b[38;5;28mdir\u001b[39m,expert\u001b[38;5;241m=\u001b[39mexpert,json_file\u001b[38;5;241m=\u001b[39mkfold)\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      5\u001b[0m     train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CustomDataset.__init__() missing 1 required positional argument: 'train'"
     ]
    }
   ],
   "source": [
    "##### DATASET #####\n",
    "train_data = get_PANDA(dir,expert=expert,json_file=kfold)\n",
    "train_dataset = CustomDataset(train_data)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False,\n",
    "    num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer4K(\n",
       "  (phi): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SET MODELS ####\n",
    "vit_patch = vit_small(\n",
    "    img_size=patch_size,\n",
    "    patch_size=mini_patch_size,\n",
    "    embed_dim=384,\n",
    "    mask_attn=False,\n",
    "    num_register_tokens=0,\n",
    ")\n",
    "\n",
    "vit_region = vit4k_xs(\n",
    "    img_size=region_size,\n",
    "    patch_size=patch_size,\n",
    "    input_embed_dim=384,\n",
    "    output_embed_dim=192,\n",
    "    mask_attn=False\n",
    ")\n",
    "\n",
    "state_dict = torch.load(checkpoint_256, map_location=\"cpu\",weights_only=False)\n",
    "checkpoint_key = \"teacher\"\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(vit_patch.state_dict(), state_dict)\n",
    "vit_patch.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_patch.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_patch.to(device)\n",
    "vit_patch.eval()\n",
    "\n",
    "state_dict = torch.load(checkpoint_4k, map_location=\"cpu\",weights_only=False)\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(\n",
    "    vit_region.state_dict(), state_dict\n",
    ")\n",
    "vit_region.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_region.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_region.to(device)\n",
    "vit_region.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GET FEATURES ####\n",
    "features = []\n",
    "labels = []\n",
    "for _,img,label in loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        feat,label = get_patches(img,vit_patch,vit_region,patch_size,region_size,y=label,mode='both',region=True)\n",
    "        features.extend(feat.cpu().detach().numpy())\n",
    "        labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "features_aux = []\n",
    "labels_aux = []\n",
    "idx_aux = []\n",
    "for i,lbl in enumerate(labels):\n",
    "    feat = features[i]\n",
    "    for j,label in enumerate(lbl):\n",
    "            features_aux.append(feat[j])\n",
    "            idx_aux.append(f'{i}_{j}')\n",
    "            labels_aux.append(np.unique(label))\n",
    "\n",
    "#### PCA ####\n",
    "pca = PCA(n_components=0.9)\n",
    "principalComponents = pca.fit_transform(features_aux)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "total_variance = sum(list(explained_variance))*100\n",
    "\n",
    "\n",
    "knn_features = []\n",
    "knn_labels = []\n",
    "knn_idx = []\n",
    "for i,lbl in enumerate(labels_aux):\n",
    "    if (2 in lbl and 3 in lbl) or (2 in lbl and 5 in lbl) or (3 in lbl and 4 in lbl) or (3 in lbl and 5 in lbl) or (4 in lbl and 5 in lbl):\n",
    "        pass\n",
    "    else:\n",
    "        knn_features.append(principalComponents[i])\n",
    "        knn_idx.append(idx_aux[i])\n",
    "        if len(lbl)==1:\n",
    "            knn_labels.append(lbl[0])\n",
    "        else:\n",
    "            knn_labels.append(lbl[-1])\n",
    "\n",
    "\n",
    "###### DIVIDE BY CLASSES #####\n",
    "cl_0 = {}\n",
    "cl_1 = {}\n",
    "cl_2 = {}\n",
    "cl_3 = {}\n",
    "cl_4 = {}\n",
    "cl_5 = {}\n",
    "for i, idx in enumerate(knn_idx):\n",
    "    div_index = idx.split('_')\n",
    "    expert_ann = labels[int(div_index[0])][int(div_index[1])]\n",
    "    cl,count = np.unique(expert_ann,return_counts=True)\n",
    "    cl_counts = dict(zip(cl,count))\n",
    "    if len(cl)==1 and cl[0]==0:\n",
    "        cl_0[idx] = knn_features[i]\n",
    "    elif knn_labels[i] == 1 :\n",
    "        if cl_counts[1]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_1[idx] = knn_features[i]\n",
    "    elif knn_labels[i] == 2:\n",
    "        if cl_counts[2]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_2[idx] = knn_features[i]\n",
    "    elif knn_labels[i] == 3:\n",
    "        if cl_counts[3]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_3[idx] = knn_features[i] \n",
    "    elif knn_labels[i] == 4:\n",
    "        if cl_counts[4]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_4[idx] = knn_features[i] \n",
    "    elif knn_labels[i] == 5:\n",
    "        if cl_counts[5]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_5[idx] = knn_features[i] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DISTANCES #######\n",
    "dist1 = calculate_distances(cl_1)\n",
    "dist2 = calculate_distances(cl_2)\n",
    "dist3 = calculate_distances(cl_3)\n",
    "dist4 = calculate_distances(cl_4)\n",
    "dist5 = calculate_distances(cl_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "##### ORIGINAL ######\n",
    "\n",
    "centroids,labels,eps,min_samples = get_core_expert([cl_1,cl_2,cl_3,cl_4,cl_5],[dist1,dist2,dist3,dist4,dist5])\n",
    "sc,sc2,eucl_distances,min_dist,outliers,dists,means,stds,_ = get_metrics(centroids,labels)\n",
    "# noisy = get_noisy(labels)\n",
    "# nn_noisy = get_nn_noisy([cl_1,cl_2,cl_3,cl_4,cl_5],centroids,labels)\n",
    "weights,percentage = get_weights(sc,outliers,dists, n_classes=n_classes)\n",
    "np.save(f'{dir}expert_masks/weights{expert}_v2.npy',weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
