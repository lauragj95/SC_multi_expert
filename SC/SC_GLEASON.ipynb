{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from source.datasets import get_Gleason, CustomDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from source.vision_transformer import vit_small, vit4k_xs\n",
    "from source.utils import update_state_dict\n",
    "from utils.dbscan_utils import get_core_expert,get_metrics, get_weights,get_noisy\n",
    "from utils.distance_utils import calculate_distances\n",
    "from utils.patching_utils import get_patches\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "    print(device)\n",
    "\n",
    "dir = '/datadisk/datasets/Gleason19/'\n",
    "img = ''\n",
    "patch_size = 256\n",
    "region_size = 4096\n",
    "mini_patch_size = 16\n",
    "n_classes = 4\n",
    "expert = 1\n",
    "json_file = 'kfolds.json'\n",
    "checkpoint_256 = '/checkpoints/vit_256_small_dino_fold_4.pt'\n",
    "checkpoint_4k = '/checkpoints/vit_4096_xs_dino_fold_4.pt'\n",
    "\n",
    "imgs = get_Gleason(dir)\n",
    "imgs_dataset = CustomDataset(imgs, False)\n",
    "\n",
    "imgs_loader = torch.utils.data.DataLoader(imgs_dataset, batch_size=1, shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer4K(\n",
       "  (phi): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SET MODELS ####\n",
    "vit_patch = vit_small(\n",
    "    img_size=patch_size,\n",
    "    patch_size=mini_patch_size,\n",
    "    embed_dim=384,\n",
    "    mask_attn=False,\n",
    "    num_register_tokens=0,\n",
    ")\n",
    "\n",
    "vit_region = vit4k_xs(\n",
    "    img_size=region_size,\n",
    "    patch_size=patch_size,\n",
    "    input_embed_dim=384,\n",
    "    output_embed_dim=192,\n",
    "    mask_attn=False\n",
    ")\n",
    "\n",
    "state_dict = torch.load(checkpoint_256, map_location=\"cpu\",weights_only=False)\n",
    "checkpoint_key = \"teacher\"\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(vit_patch.state_dict(), state_dict)\n",
    "vit_patch.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_patch.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_patch.to(device)\n",
    "vit_patch.eval()\n",
    "\n",
    "state_dict = torch.load(checkpoint_4k, map_location=\"cpu\",weights_only=False)\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(\n",
    "    vit_region.state_dict(), state_dict\n",
    ")\n",
    "vit_region.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_region.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_region.to(device)\n",
    "vit_region.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GET FEATURES ####\n",
    "features = []\n",
    "\n",
    "for img in imgs_loader:\n",
    "        img = img[1].to(device)\n",
    "        \n",
    "        feat = get_patches(img,vit_patch,vit_region,patch_size,region_size)\n",
    "        features.extend(feat.cpu().detach().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sc_expert(expert,features):\n",
    "    #### DATA LOADER ####  \n",
    "    train_data = get_Gleason(dir,expert=expert,mode=\"label\")\n",
    "    #### GET FEATURES ####\n",
    "    labels = []\n",
    "    for label in train_data[0]:\n",
    "            label = get_patches(torch.from_numpy(label).to(device),vit_patch,vit_region,patch_size,region_size,mode=\"label\")\n",
    "            labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    features_aux = []\n",
    "    labels_aux = []\n",
    "    idx_aux = []\n",
    "\n",
    "    for i,lbl in enumerate(labels):\n",
    "        feat = features[i]\n",
    "        for j,label in enumerate(lbl):\n",
    "                features_aux.append(feat[j])\n",
    "                idx_aux.append(f'{i}_{j}')\n",
    "                labels_aux.append(np.unique(label))\n",
    "\n",
    "    #### PCA ####\n",
    "    pca = PCA(n_components=0.9)\n",
    "    principalComponents = pca.fit_transform(features_aux)\n",
    "\n",
    "\n",
    "    knn_features = []\n",
    "    knn_labels = []\n",
    "    knn_idx = []\n",
    "    for i,lbl in enumerate(labels_aux):\n",
    "        if len(lbl)==1:\n",
    "            if lbl[0]!=-1:\n",
    "                knn_features.append(principalComponents[i])\n",
    "                knn_idx.append(idx_aux[i])\n",
    "                knn_labels.append(lbl)\n",
    "        else:\n",
    "            if -1 not in lbl:\n",
    "                knn_features.append(principalComponents[i])\n",
    "                knn_idx.append(idx_aux[i])\n",
    "                knn_labels.append(lbl)\n",
    "            \n",
    "    ###### DIVIDE BY CLASSES #####\n",
    "    cl_1 = {}\n",
    "    cl_3 = {}\n",
    "    cl_4 = {}\n",
    "    cl_5 = {}\n",
    "\n",
    "\n",
    "    for i, idx in enumerate(knn_idx):\n",
    "        div_index = idx.split('_')\n",
    "        expert_ann = labels[int(div_index[0])][int(div_index[1])]\n",
    "        cl,count = np.unique(expert_ann,return_counts=True)\n",
    "        cl_counts = dict(zip(cl,count))\n",
    "\n",
    "        if 1 in knn_labels[i]:\n",
    "            if cl_counts[1]/(256**2)>0.8:\n",
    "                cl_1[idx] = knn_features[i]\n",
    "        if 2 in knn_labels[i]:\n",
    "            if cl_counts[2]/(256**2)>0.8:\n",
    "                cl_1[idx] = knn_features[i]\n",
    "        if 3 in knn_labels[i]:\n",
    "            if cl_counts[3]/(256**2)>0.8:\n",
    "                cl_3[idx] = knn_features[i] \n",
    "        if 4 in knn_labels[i]:\n",
    "            if cl_counts[4]/(256**2)>0.8:\n",
    "                cl_4[idx] = knn_features[i] \n",
    "        if 5 in knn_labels[i]:\n",
    "            if cl_counts[5]/(256**2)>0.8:\n",
    "                cl_5[idx] = knn_features[i] \n",
    "    ###### DISTANCES #######\n",
    "\n",
    "    dist1 = calculate_distances(cl_1)\n",
    "    dist3 = calculate_distances(cl_3)\n",
    "    dist4 = calculate_distances(cl_4)\n",
    "    dist5 = calculate_distances(cl_5)\n",
    "\n",
    "\n",
    "    ######## DBSCAN #########\n",
    "\n",
    "    centroids,labels,eps,min_samples = get_core_expert([cl_1,cl_3,cl_4,cl_5],[dist1,dist3,dist4,dist5])\n",
    "    sc,sc2,eucl_distances,min_dist,outliers,dists,means,stds,_ =  get_metrics(centroids,labels,return_neighbors=False)\n",
    "\n",
    "    noisy = []\n",
    "    for label in labels:\n",
    "        noisy.append(get_noisy(label))\n",
    "    n_centroids = [centroid.shape[0] for centroid in centroids]\n",
    "\n",
    "\n",
    "    weights,percentage = get_weights(sc,outliers,dists,n_classes=n_classes)\n",
    "    # np.save(f'{dir}/weights/weights{expert}_original.npy',weights)\n",
    "    \n",
    "    return [len(cl_1),len(cl_3),len(cl_4),len(cl_5)],n_centroids,noisy,sc,min_dist,\\\n",
    "        outliers,dists,means,stds,weights,percentage,eps,min_samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cl1,n_centroids1,noisy1,sc_exp1,min_edist_exp1,outliers_exp1,edists_exp1,\\\n",
    "means_exp1,stds_exp1,sc2_exp1,eucl_distance_exp1,\\\n",
    "    weights_exp1,percentage_exp1,eps1,min_samples1 = get_sc_expert(1,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cl1,n_centroids1,noisy1,sc_exp1,min_edist_exp1,outliers_exp1,edists_exp1,\\\n",
    "means_exp1,stds_exp1,sc2_exp1,eucl_distance_exp1,\\\n",
    "    weights_exp1,percentage_exp1,eps1,min_samples1 = get_sc_expert(1,features)\n",
    "    \n",
    "len_cl2,n_centroids2,noisy2,sc_exp2,min_edist_exp2,outliers_exp2,edists_exp2,\\\n",
    "means_exp2,stds_exp2,\\\n",
    "    weights_exp2,percentage_exp2,eps2,min_samples2 = get_sc_expert(2,features) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "len_cl3,n_centroids3,noisy3,sc_exp3,min_edist_exp3,outliers_exp3,edists_exp3,\\\n",
    "means_exp3,stds_exp3,sc2_exp3,eucl_distance_exp3,\\\n",
    "    weights_exp3,percentage_exp3,eps3,min_samples3 = get_sc_expert(3,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cl4,n_centroids4,noisy4,sc_exp4,min_edist_exp4,outliers_exp4,edists_exp4,\\\n",
    "means_exp4,stds_exp4,sc2_exp4,eucl_distance_exp4,\\\n",
    "    weights_exp4,percentage_exp4,eps4,min_samples4 = get_sc_expert(4,features)  \n",
    "    \n",
    "    \n",
    "len_cl5,n_centroids5,noisy5,sc_exp5,min_edist_exp5,outliers_exp5,edists_exp5,\\\n",
    "means_exp5,stds_exp5,sc2_exp5,eucl_distance_exp5,\\\n",
    "    weights_exp5,percentage_exp5,eps5,min_samples5 = get_sc_expert(5,features)\n",
    "    \n",
    "len_cl6,n_centroids6,noisy6,sc_exp6,min_edist_exp6,outliers_exp6,edists_exp6,\\\n",
    "means_exp6,stds_exp6,sc2_exp6,eucl_distance_exp6,\\\n",
    "    weights_exp6,percentage_exp6,eps6,min_samples6 = get_sc_expert(6,features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
