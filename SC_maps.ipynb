{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from source.dataset_gleason import get_PANDA, GleasonDataset\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from source.vision_transformer import vit_small, vit4k_xs\n",
    "from source.utils import update_state_dict\n",
    "import random \n",
    "import copy \n",
    "from utils.dbscan_utils import get_core_expert,get_metrics,get_nn_noisy,get_weights,get_noisy\n",
    "from utils.patching_utils import get_patches\n",
    "from utils.distance_utils import calculate_distances\n",
    "from utils.experts_utils import create_expert\n",
    "\n",
    "from skimage.io import imsave\n",
    "import write_results\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "    print(device)\n",
    "    \n",
    "random.seed(0)\n",
    "dir = '/home/laura/Documents/dataset/PANDA/'\n",
    "img = ''\n",
    "patch_size = 256\n",
    "region_size = 4096\n",
    "mini_patch_size = 16\n",
    "checkpoint_256 = 'checkpoints/vit_256_small_dino_fold_4.pt'\n",
    "checkpoint_4k = 'checkpoints/vit_4096_xs_dino_fold_4.pt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATASET #####\n",
    "train_data = get_PANDA(dir)\n",
    "train_dataset = GleasonDataset(train_data, False)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False,\n",
    "    num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267657/2652635714.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_256, map_location=\"cpu\")\n",
      "/tmp/ipykernel_267657/2652635714.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_4k, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer4K(\n",
       "  (phi): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SET MODELS ####\n",
    "vit_patch = vit_small(\n",
    "    img_size=patch_size,\n",
    "    patch_size=mini_patch_size,\n",
    "    embed_dim=384,\n",
    "    mask_attn=False,\n",
    "    num_register_tokens=0,\n",
    ")\n",
    "\n",
    "vit_region = vit4k_xs(\n",
    "    img_size=region_size,\n",
    "    patch_size=patch_size,\n",
    "    input_embed_dim=384,\n",
    "    output_embed_dim=192,\n",
    "    mask_attn=False\n",
    ")\n",
    "\n",
    "state_dict = torch.load(checkpoint_256, map_location=\"cpu\")\n",
    "checkpoint_key = \"teacher\"\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(vit_patch.state_dict(), state_dict)\n",
    "vit_patch.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_patch.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_patch.to(device)\n",
    "vit_patch.eval()\n",
    "\n",
    "state_dict = torch.load(checkpoint_4k, map_location=\"cpu\")\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "state_dict, msg = update_state_dict(\n",
    "    vit_region.state_dict(), state_dict\n",
    ")\n",
    "vit_region.load_state_dict(state_dict, strict=False)\n",
    "for name, param in vit_region.named_parameters():\n",
    "    param.requires_grad = False\n",
    "vit_region.to(device)\n",
    "vit_region.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GET FEATURES ####\n",
    "features = []\n",
    "labels = []\n",
    "for _,img,label in loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        feat,label = get_patches(img,label,vit_patch,vit_region,patch_size,region_size)\n",
    "        features.extend(feat.cpu().detach().numpy())\n",
    "        labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "features_aux = []\n",
    "labels_aux = []\n",
    "idx_aux = []\n",
    "for i,lbl in enumerate(labels):\n",
    "    feat = features[i]\n",
    "    for j,label in enumerate(lbl):\n",
    "            features_aux.append(feat[j])\n",
    "            idx_aux.append(f'{i}_{j}')\n",
    "            labels_aux.append(np.unique(label))\n",
    "\n",
    "#### PCA ####\n",
    "pca = PCA(n_components=0.9)\n",
    "principalComponents = pca.fit_transform(features_aux)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "total_variance = sum(list(explained_variance))*100\n",
    "\n",
    "\n",
    "knn_features = []\n",
    "knn_labels = []\n",
    "knn_idx = []\n",
    "for i,lbl in enumerate(labels_aux):\n",
    "    if (2 in lbl and 3 in lbl) or (2 in lbl and 5 in lbl) or (3 in lbl and 4 in lbl) or (3 in lbl and 5 in lbl) or (4 in lbl and 5 in lbl):\n",
    "        pass\n",
    "    else:\n",
    "        knn_features.append(principalComponents[i])\n",
    "        knn_idx.append(idx_aux[i])\n",
    "        if len(lbl)==1:\n",
    "            knn_labels.append(lbl[0])\n",
    "        else:\n",
    "            knn_labels.append(lbl[-1])\n",
    "\n",
    "\n",
    "###### DIVIDE BY CLASSES #####\n",
    "cl_0 = {}\n",
    "cl_1 = {}\n",
    "cl_2 = {}\n",
    "cl_3 = {}\n",
    "cl_4 = {}\n",
    "cl_5 = {}\n",
    "for i, idx in enumerate(knn_idx):\n",
    "    div_index = idx.split('_')\n",
    "    expert_ann = labels[int(div_index[0])][int(div_index[1])]\n",
    "    cl,count = np.unique(expert_ann,return_counts=True)\n",
    "    cl_counts = dict(zip(cl,count))\n",
    "    if len(cl)==1 and cl[0]==0:\n",
    "        cl_0[idx] = knn_features[i]\n",
    "    elif knn_labels[i] == 1 :\n",
    "        if cl_counts[1]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_1[idx] = knn_features[i]\n",
    "    elif knn_labels[i] == 2:\n",
    "        if cl_counts[2]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_2[idx] = knn_features[i]\n",
    "    elif knn_labels[i] == 3:\n",
    "        if cl_counts[3]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_3[idx] = knn_features[i] \n",
    "    elif knn_labels[i] == 4:\n",
    "        if cl_counts[4]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_4[idx] = knn_features[i] \n",
    "    elif knn_labels[i] == 5:\n",
    "        if cl_counts[5]/(256**2)>0.15 and 0 not in cl:\n",
    "            cl_5[idx] = knn_features[i] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DISTANCES #######\n",
    "dist0 = calculate_distances(cl_0)\n",
    "dist1 = calculate_distances(cl_1)\n",
    "dist2 = calculate_distances(cl_2)\n",
    "dist3 = calculate_distances(cl_3)\n",
    "dist4 = calculate_distances(cl_4)\n",
    "dist5 = calculate_distances(cl_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "##### ORIGINAL ######\n",
    "\n",
    "centroids_orig,labels_orig,eps_orig,min_samples_orig = get_core_expert([cl_1,cl_2,cl_3,cl_4,cl_5],[dist1,dist2,dist3,dist4,dist5])\n",
    "sc_orig,sc2_orig,eucl_distances_orig,min_dist_orig,outliers_orig,dists_orig,means_orig,stds_orig,neigbors_orig = get_metrics(centroids_orig,labels_orig)\n",
    "noisy_orig = get_noisy(labels_orig)\n",
    "nn_noisy_orig = get_nn_noisy([cl_1,cl_2,cl_3,cl_4,cl_5],centroids_orig,labels_orig)\n",
    "weights_orig,percentage_orig = get_weights(sc_orig,outliers_orig,dists_orig)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weightsOrig_percentile.npy',weights_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "###### EXPERT 1 ########\n",
    "cls_exp1,new_dists_exp1 = create_expert(1,{2:[3],3:[2]},{2:cl_2,3:cl_3},labels,train_data[2],0.2,0)\n",
    "centroids_exp1,labels_exp1,eps_exp1,min_samples_exp1 = get_core_expert([cl_1,cls_exp1[2],cls_exp1[3],cl_4,cl_5],[dist1,new_dists_exp1[2],new_dists_exp1[3],dist4,dist5])\n",
    "sc_exp1,sc2_exp1,eucl_distances_exp1,min_dist_exp1,outliers_exp1,dists_exp1,means_exp1,stds_exp1,neigbors_exp1 = get_metrics(centroids_exp1,labels_exp1)\n",
    "noisy_exp1 = get_noisy(labels_exp1)\n",
    "nn_noisy_exp1 = get_nn_noisy([cl_1,cls_exp1[2],cls_exp1[3],cl_4,cl_5],centroids_exp1,labels_exp1)\n",
    "weights_exp1,percentage_exp1 = get_weights(sc_exp1,outliers_exp1,dists_exp1)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weights1_percentile.npy',weights_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "###### EXPERT 2 ########\n",
    "cls_exp2,new_dists_exp2 = create_expert(2,{4:[3],3:[4]},{3:cl_3,4:cl_4},labels,train_data[2],0.2,1)\n",
    "centroids_exp2,labels_exp2,eps_exp2,min_samples_exp2 = get_core_expert([cl_1,cl_2,cls_exp2[3],cls_exp2[4],cl_5],[dist1,dist2,new_dists_exp2[3],new_dists_exp2[4],dist5])\n",
    "sc_exp2,sc2_exp2,eucl_distances_exp2,min_dist_exp2,outliers_exp2,dists_exp2,means_exp2,stds_exp2,neigbors_exp2 = get_metrics(centroids_exp2,labels_exp2)\n",
    "noisy_exp2 = get_noisy(labels_exp2)\n",
    "nn_noisy_exp2 = get_nn_noisy([cl_1,cl_2,cls_exp2[3],cls_exp2[4],cl_5],centroids_exp2,labels_exp2)\n",
    "weights_exp2,percentage_exp2 = get_weights(sc_exp2,outliers_exp2,dists_exp2)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weights2_percentile.npy',weights_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_expert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/laura/Documents/GitHub/multi-expert-transformer/SC_maps.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bviolette/home/laura/Documents/GitHub/multi-expert-transformer/SC_maps.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m###### EXPERT 3 ########\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bviolette/home/laura/Documents/GitHub/multi-expert-transformer/SC_maps.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cls_exp3,new_dists_exp3 \u001b[39m=\u001b[39m create_expert(\u001b[39m3\u001b[39m,{\u001b[39m2\u001b[39m:[\u001b[39m3\u001b[39m],\u001b[39m4\u001b[39m:[\u001b[39m3\u001b[39m],\u001b[39m3\u001b[39m:[\u001b[39m2\u001b[39m,\u001b[39m4\u001b[39m]},{\u001b[39m2\u001b[39m:cl_2,\u001b[39m3\u001b[39m:cl_3,\u001b[39m4\u001b[39m:cl_4},labels,train_data[\u001b[39m2\u001b[39m],\u001b[39m0.15\u001b[39m,\u001b[39m47\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bviolette/home/laura/Documents/GitHub/multi-expert-transformer/SC_maps.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m centroids_exp3,labels_exp3,eps_exp3,min_samples_exp3 \u001b[39m=\u001b[39m get_core_expert([cl_1,cls_exp3[\u001b[39m2\u001b[39m],cls_exp3[\u001b[39m3\u001b[39m],cls_exp3[\u001b[39m4\u001b[39m],cl_5],[dist1,new_dists_exp3[\u001b[39m2\u001b[39m],new_dists_exp3[\u001b[39m3\u001b[39m],new_dists_exp3[\u001b[39m4\u001b[39m],dist5])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bviolette/home/laura/Documents/GitHub/multi-expert-transformer/SC_maps.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m sc_exp3,sc2_exp3,eucl_distances_exp3,min_dist_exp3,outliers_exp3,dists_exp3,means_exp3,stds_exp3,neigbors_exp3 \u001b[39m=\u001b[39m get_metrics(centroids_exp3,labels_exp3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_expert' is not defined"
     ]
    }
   ],
   "source": [
    "###### EXPERT 3 ########\n",
    "cls_exp3,new_dists_exp3 = create_expert(3,{2:[3],4:[3],3:[2,4]},{2:cl_2,3:cl_3,4:cl_4},labels,train_data[2],0.15,47)\n",
    "centroids_exp3,labels_exp3,eps_exp3,min_samples_exp3 = get_core_expert([cl_1,cls_exp3[2],cls_exp3[3],cls_exp3[4],cl_5],[dist1,new_dists_exp3[2],new_dists_exp3[3],new_dists_exp3[4],dist5])\n",
    "sc_exp3,sc2_exp3,eucl_distances_exp3,min_dist_exp3,outliers_exp3,dists_exp3,means_exp3,stds_exp3,neigbors_exp3 = get_metrics(centroids_exp3,labels_exp3)\n",
    "noisy_exp3 = get_noisy(labels_exp3)\n",
    "nn_noisy_exp3 = get_nn_noisy([cl_1,cls_exp3[2],cls_exp3[3],cls_exp3[4],cl_5],centroids_exp3,labels_exp3)\n",
    "weights_exp3,percentage_exp3 = get_weights(sc_exp3,outliers_exp3,dists_exp3)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weights3_percentile.npy',weights_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "###### EXPERT 4 ########\n",
    "cls_exp4,new_dists_exp4 = create_expert(4,{2:[3],3:[2]},{2:cl_2,3:cl_3},labels,train_data[2],0.25,10)\n",
    "centroids_exp4,labels_exp4,eps_exp4,min_samples_exp4 = get_core_expert([cl_1,cls_exp4[2],cls_exp4[3],cl_4,cl_5],[dist1,new_dists_exp4[2],new_dists_exp4[3],dist4,dist5])\n",
    "sc_exp4,sc2_exp4,eucl_distances_exp4,min_dist_exp4,outliers_exp4,dists_exp4,means_exp4,stds_exp4,neigbors_exp4 = get_metrics(centroids_exp4,labels_exp4)\n",
    "nn_noisy_exp4 = get_nn_noisy([cl_1,cls_exp4[2],cls_exp4[3],cl_4,cl_5],centroids_exp4,labels_exp4)\n",
    "noisy_exp4 = get_noisy(labels_exp4)\n",
    "weights_exp4,percentage_exp4 = get_weights(sc_exp4,outliers_exp4,dists_exp4)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weights4_percentile.npy',weights_exp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "###### EXPERT 5 ########\n",
    "cls_exp5,new_dists_exp5 = create_expert(5,{4:[3],3:[4]},{3:cl_3,4:cl_4},labels,train_data[2],0.85,0)\n",
    "centroids_exp5,labels_exp5,eps_exp5,min_samples_exp5 = get_core_expert([cl_1,cl_2,cls_exp5[3],cls_exp5[4],cl_5],[dist1,dist2,new_dists_exp5[3],new_dists_exp5[4],dist5])\n",
    "sc_exp5,sc2_exp5,eucl_distances_exp5,min_dist_exp5,outliers_exp5,dists_exp5,means_exp5,stds_exp5,neigbors_exp5 = get_metrics(centroids_exp5,labels_exp5)\n",
    "noisy_exp5 = get_noisy(labels_exp5)\n",
    "nn_noisy_exp5 = get_nn_noisy([cl_1,cl_2,cls_exp5[3],cls_exp5[4],cl_5],centroids_exp5,labels_exp5)\n",
    "weights_exp5,percentage_exp5 = get_weights(sc_exp5,outliers_exp5,dists_exp5)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weights5_percentile.npy',weights_exp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/envs/multi/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "###### EXPERT 6 ########\n",
    "cls_exp6,new_dists_exp6 = create_expert(6,{2:[3],4:[3],3:[2,4]},{2:cl_2,3:cl_3,4:cl_4},labels,train_data[2],0.1,5)\n",
    "centroids_exp6,labels_exp6,eps_exp6,min_samples_exp6 = get_core_expert([cl_1,cls_exp6[2],cls_exp6[3],cls_exp6[4],cl_5],[dist1,new_dists_exp6[2],new_dists_exp6[3],new_dists_exp6[4],dist5])\n",
    "sc_exp6,sc2_exp6,eucl_distances_exp6,min_dist_exp6,outliers_exp6,dists_exp6,means_exp6,stds_exp6,neigbors_exp6 = get_metrics(centroids_exp6,labels_exp6)\n",
    "noisy_exp6 = get_noisy(labels_exp6)\n",
    "nn_noisy_exp6 = get_nn_noisy([cl_1,cls_exp6[2],cls_exp6[3],cls_exp6[4],cl_5],centroids_exp6,labels_exp6)\n",
    "weights_exp6,percentage_exp6 = get_weights(sc_exp6,outliers_exp6,dists_exp6)\n",
    "# np.save('/home/laura/Documents/dataset/PANDA/expert_masks/weights6_percentile.npy',weights_exp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = [eps_orig,eps_exp1,eps_exp2,eps_exp3,eps_exp4,eps_exp5,eps_exp6]\n",
    "min_samples = [min_samples_orig,min_samples_exp1,min_samples_exp2,min_samples_exp3,min_samples_exp4,min_samples_exp5,min_samples_exp6]\n",
    "centroids = [centroids_orig,centroids_exp1,centroids_exp2,centroids_exp3,centroids_exp4,centroids_exp5,centroids_exp6]\n",
    "nn_noisy = [nn_noisy_orig,nn_noisy_exp1,nn_noisy_exp2,nn_noisy_exp3,nn_noisy_exp4,nn_noisy_exp5,nn_noisy_exp6]\n",
    "noisy = [noisy_orig,noisy_exp1,noisy_exp2,noisy_exp3,noisy_exp4,noisy_exp5,noisy_exp6]\n",
    "nn = [neigbors_orig,neigbors_exp1,neigbors_exp2,neigbors_exp3,neigbors_exp4,neigbors_exp5,neigbors_exp6]\n",
    "outliers = [outliers_orig,outliers_exp1,outliers_exp2,outliers_exp3,outliers_exp4,outliers_exp5,outliers_exp6]\n",
    "means = [means_orig,means_exp1,means_exp2,means_exp3,means_exp4,means_exp5,means_exp6]\n",
    "stds = [stds_orig,stds_exp1,stds_exp2,stds_exp3,stds_exp4,stds_exp5,stds_exp6]\n",
    "e_distance = [dists_orig,dists_exp1,dists_exp2,dists_exp3,dists_exp4,dists_exp5,dists_exp6]\n",
    "dist_sc_v2 = [eucl_distances_orig,eucl_distances_exp1,eucl_distances_exp2,eucl_distances_exp3,eucl_distances_exp4,eucl_distances_exp5,eucl_distances_exp6]\n",
    "weights = [weights_orig,weights_exp1,weights_exp2,weights_exp3,weights_exp4,weights_exp5,weights_exp6]\n",
    "percentage = [percentage_orig,percentage_exp1,percentage_exp2,percentage_exp3,percentage_exp4,percentage_exp5,percentage_exp6]\n",
    "sc_v2 = [sc2_orig,sc2_exp1,sc2_exp2,sc2_exp3,sc2_exp4,sc2_exp5,sc2_exp6]\n",
    "sc_v1 = [sc_orig,sc_exp1,sc_exp2,sc_exp3,sc_exp4,sc_exp5,sc_exp6]\n",
    "min_e_distance = [min_dist_orig,min_dist_exp1,min_dist_exp2,min_dist_exp3,min_dist_exp4,min_dist_exp5,min_dist_exp6]\n",
    "\n",
    "cl_len_orig = [len(cl_1),len(cl_2),len(cl_3),len(cl_4),len(cl_5)]\n",
    "cl_len_exp1 = [len(cl_1),len(cls_exp1[2]),len(cls_exp1[3]),len(cl_4),len(cl_5)]\n",
    "cl_len_exp2 = [len(cl_1),len(cl_2),len(cls_exp2[3]),len(cls_exp2[4]),len(cl_5)]\n",
    "cl_len_exp3 = [len(cl_1),len(cls_exp3[2]),len(cls_exp3[3]),len(cls_exp3[4]),len(cl_5)]\n",
    "cl_len_exp4 = [len(cl_1),len(cls_exp4[2]),len(cls_exp4[3]),len(cl_4),len(cl_5)]\n",
    "cl_len_exp5 = [len(cl_1),len(cl_2),len(cls_exp5[3]),len(cls_exp5[4]),len(cl_5)]\n",
    "cl_len_exp6 = [len(cl_1),len(cls_exp6[2]),len(cls_exp6[3]),len(cls_exp6[4]),len(cl_5)]\n",
    "cl_len = [cl_len_orig,cl_len_exp1,cl_len_exp2,cl_len_exp3,cl_len_exp4,cl_len_exp5,cl_len_exp6]\n",
    "\n",
    "experts = ['original','exp1','exp2','exp3','exp4','exp5','exp6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import write_results\n",
    "cl = ['cl1','cl2','cl3','cl4','cl5']\n",
    "write_results.write_cores(nn,centroids,cl_len,experts=experts)\n",
    "write_results.write_noisy(nn_noisy,noisy,cl_len,experts=experts)\n",
    "write_results.write_outliers(outliers,experts=experts)\n",
    "write_results.write_euclidean_distances(means,stds,experts=experts)\n",
    "write_results.write_edistances(e_distance,experts=experts)\n",
    "write_results.write_min_euclidean_distance(dist_sc_v2,experts=experts)\n",
    "write_results.write_min_edistance(min_e_distance,experts=experts)\n",
    "write_results.write_sc1(sc_v1,experts=experts)\n",
    "write_results.write_sc2(sc_v2,experts=experts)\n",
    "write_results.write_weights(weights,percentage,experts=experts)\n",
    "write_results.write_parameters(cl_len,eps,min_samples,experts=experts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
